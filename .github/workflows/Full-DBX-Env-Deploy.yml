# yaml-language-server: $schema=https://json.schemastore.org/github-workflow.json

#https://github.com/marketplace/actions/run-databricks-notebook


# TO DO: Write A New Package For Setting Environment Variables From A Parameters File

name: DBX_CICD-DEPLOY

on: push

permissions:
      id-token:               write
      contents:               read

jobs:
  DBX_CICD_Deployment:
      name:                     DBX_CICD_Deployment
      runs-on:                  ubuntu-latest
      strategy:
        matrix:
          environments:          [Development, UAT, PreProduction, Production]    
      steps:
        - uses:                  actions/checkout@v3

        # az ad sp create-for-rbac -n DevOpsAgentSP --role Owner --scopes /subscriptions/4f1bc772-7792-4285-99d9-3463b8d7f994 --sdk-auth
        
        # Login To testappciaran. This Has Higher Privileges/ Ownership To Create Resources/RBACs. We Generally Wouldn't Be Given 
        # SP Secrets etc. As This Would Confer Too Much Power. Super Users Will Use This Login. DBX SP should have contributor rights.
        
        # It is important to set up federated connections (below)
        # https://goodworkaround.com/2021/12/21/another-deep-dive-into-azure-ad-workload-identity-federation-using-github-actions/


        - name:                 Azure Login - ${{ matrix.environments }}
          uses:                 azure/login@v1
          with:
            creds:              ${{secrets.AZURE_CREDENTIALS}}
      

        - name:                 Store JSON Param File Variables As Environ Variables
          uses:                 antifree/json-to-variables@v1.0.1
          with:
            filename:           '.github/workflows/Pipeline_Param/${{ matrix.environments }}.json'
            prefix:             param


        #- name:                 Deploy DBX CICD Azure Resources
        #  run:                  bash ./.github/workflows/Utilities/Utils-Azure-Resources.sh
        #  env:
        #    environment:        ${{ matrix.environments }}


        - name:                 Assign RBAC Permissions 
          run:                  sh ./.github/workflows/Utilities/Utils-Assign-RBAC.sh
          env:
            environment:        ${{ matrix.environments }}

        # Switch To DBX SP.

        # Day To Day Use Interacting With Databricks API Does Not Require God Rights dbxsp. The Principal of Zero Trust Applies.
        
        # Therefore We Use The DBX SP (Only Has Databricks Custom Role Assignments - No Owner Permissions etc), For Interacting With Databricks...
        # API To Create Clusters/ Jobs etc. Might be worth giving sp contributor right


        - name:                 Authenticate to DBX Service Principal
          run:                  bash ./.github/workflows/Utilities/Utils-DBX-SP-Authenticate.sh
          env:
            ARM_CLIENT_ID:      ${{ secrets.ARM_CLIENT_ID }}
            ARM_CLIENT_SECRET:  ${{ secrets.ARM_CLIENT_SECRET }}
            ARM_TENANT_ID:      ${{ secrets.ARM_TENANT_ID }}

            
        - name:                 Setup Python
          uses:                 actions/setup-python@v4
          with:
            python-version:     '3.8'


        - name:                 Create And Store PAT Token In Key Vault
          run:                  bash ./.github/workflows/Utilities/Utils-Create-PAToken.sh


        - name:                 Create DBX Secret Scopes
          run:                  bash ./.github/workflows/Utilities/Utils-Create-Scope.sh
        

        - name:                 Create DBX Clusters
          run:                  bash ./.github/workflows/Utilities/Utils-Create-Cluster.sh
          env:
            environment:        ${{ matrix.environments }}


        - name:                 Create DBX Repos
          run:                  sh ./.github/workflows/Utilities/Utils-Create-Repo-Folders.sh
          env:
            environment:        ${{ matrix.environments }}
            PAT_GIT:            ${{ secrets.PAT_GIT }}


        - name:                   Install + Configure Databricks CLI
          run:                    bash ./.github/workflows/Utilities/Utils-DBX-CLI-Configure.sh

        - name:                   Import Python Wheel Dependencies 
          run: | 
            python -m pip install --upgrade pip
            python -m pip install flake8 pytest pyspark pytest-cov requests

            pip3 install -r ./src/pipelines/dbkframework/requirements.txt
            python -m pip install --user --upgrade setuptools wheel


        #- name: Lint with flake8
        #  run: |
        #    # stop the build if there are Python syntax errors or undefined names
        #    flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        #    # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        #    flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
        - name: Create wheel
          run: |
            ls
            cd src/pipelines/dbkframework
            python setup.py sdist bdist_wheel

        - name: Install wheel
          run: |
            cd src/pipelines/dbkframework
            pip uninstall -y dfordbx-0.0.1-py3-none-any.whl
            pip install dfordbx-0.0.1-py3-none-any.whl
  
        - name: Upoload Wheel File To DBFS
          run: |
            # Make Wheel Name Dynamic

            databricks fs cp "src/pipelines/dbkframework/dist/dbkframework-1-py3-none-any.whl" dbfs:/FileStore/dev --overwrite
          shell: bash

        - name: Install Wheel On Cluster
          run: |
            # Get The Cluster ID & Make Wheel Name Dynamic
            
            databricks libraries install --cluster-id "0802-090441-honks846" --whl "dbfs:/FileStore/dev/dbkframework-1-py3-none-any.whl"  
          shell: bash    











































            # Navigate To The Setup.Py File.
            #ls
            #cd src
            #cd pipelines
            #cd dbkframework

            # Create The Python Wheel File
            #python setup.py sdist bdist_wheel

            #cd dist
            #echo "Name of Pyton Wheel File"
            #ls

            #databricks fs -h
            #databricks fs ls
            #databricks fs rm 'dbfs:/FileStore/dev/artifacts/'
            #databricks fs cp -r dbkframework-1-py3-none-any.whl dbfs:/FileStore/dev/artifacts/

            #databricks libraries uninstall --cluster-id {cluster_id} --whl dbfs:/FileStore/dev/artifacts/dbkframework-1-py3-none-any.whl
            #databricks libraries install --cluster-id {cluster_id} --whl dbfs:/FileStore/dev/artifacts/dbkframework-1-py3-none-any.whl

          #shell: bash

          
        
